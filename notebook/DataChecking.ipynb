{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import MeCab\n",
    "import mojimoji\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, text_path):\n",
    "        path = Path(text_path)\n",
    "        assert path.exists()\n",
    "        assert path.is_dir()\n",
    "\n",
    "        self.classes = list()\n",
    "        self.dir = list()\n",
    "        for p in path.iterdir():\n",
    "            if p.is_dir():\n",
    "                self.classes.append(p.name)\n",
    "                self.dir.append(p)\n",
    "\n",
    "        self.encode_dict = {k: i for i, k in enumerate(self.classes)}\n",
    "        self.decode_dict = {i: k for i, k in enumerate(self.classes)}\n",
    "\n",
    "        self.title = list()\n",
    "        self.url = list()\n",
    "        self.datetime = list()\n",
    "        self.text = list()\n",
    "        self.label = list()\n",
    "\n",
    "        for p in self.dir:\n",
    "            for d in p.iterdir():\n",
    "                if d.name != \"LICENSE.txt\":\n",
    "                    with open(d) as f:\n",
    "                        split = f.readlines()\n",
    "                    self.url.append(split[0].replace(\"\\n\", \"\"))\n",
    "                    self.datetime.append(split[1].replace(\"\\n\", \"\"))\n",
    "                    self.title.append(split[2].replace(\"\\n\", \"\"))\n",
    "                    self.text.append(\" \".join([\n",
    "                        s.replace(\"\\n\", \"\").replace(\"\\u3000\", \" \")\n",
    "                        for s in split[3:]\n",
    "                    ]))\n",
    "                    self.label.append(self.encode_dict[p.name])\n",
    "        self.data = pd.DataFrame({\n",
    "            \"date\": self.datetime,\n",
    "            \"url\": self.url,\n",
    "            \"title\": self.title,\n",
    "            \"text\": self.text,\n",
    "            \"label\": self.label\n",
    "        })\n",
    "\n",
    "        self.tokenized = False\n",
    "\n",
    "    def tokenize(self, tokenizer, kwargs={}):\n",
    "        self.data[\"concatenated\"] = self.data[\"title\"] + \" \" + \\\n",
    "            self.data[\"text\"]\n",
    "        self.data[\"tokenized\"] = self.data.concatenated.map(\n",
    "            lambda x: tokenizer(x, **kwargs))\n",
    "        self.tokenized = True\n",
    "\n",
    "    def load(self):\n",
    "        assert self.tokenized, \"Needs tokenization before loading\"\n",
    "        train, test = train_test_split(\n",
    "            self.data, test_size=0.3, shuffle=True, random_state=42)\n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(text_path=\"../input/text/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-10-30T10:15:00+0900</td>\n",
       "      <td>http://news.livedoor.com/article/detail/5978741/</td>\n",
       "      <td>【DVDエンター！】誘拐犯に育てられた女が目にした真実は、孤独か幸福か</td>\n",
       "      <td>2005年11月から翌2006年7月まで読売新聞にて連載された、直木賞作家・角田光代による...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-02-29T11:45:00+0900</td>\n",
       "      <td>http://news.livedoor.com/article/detail/6322901/</td>\n",
       "      <td>藤原竜也、中学生とともにロケット打ち上げに成功</td>\n",
       "      <td>「アンテナを張りながら生活をしていけばいい」   2月28日、映画『おかえり、はやぶさ』（...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-09T14:00:00+0900</td>\n",
       "      <td>http://news.livedoor.com/article/detail/6176324/</td>\n",
       "      <td>『戦火の馬』ロイヤル・プレミアにウィリアム王子＆キャサリン妃が出席</td>\n",
       "      <td>3月2日より全国ロードショーとなる、スティーブン・スピルバーグの待望の監督最新作『戦火の馬...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-05-19T12:00:00+0900</td>\n",
       "      <td>http://news.livedoor.com/article/detail/6573929/</td>\n",
       "      <td>香里奈、女子高生100人のガチンコ質問に回答「ラーメンも食べる」</td>\n",
       "      <td>女優の香里奈が18日、都内で行われた映画『ガール』（5月26日公開）の女子高生限定試写会に...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-10-05T19:11:00+0900</td>\n",
       "      <td>http://news.livedoor.com/article/detail/5914880/</td>\n",
       "      <td>ユージの前に立ちはだかったJOY「僕はAKBの高橋みなみを守る」</td>\n",
       "      <td>5日、東京・千代田区の内幸町ホールにて、映画『キャプテン・アメリカ/ザ・ファースト・アベン...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date                                               url  \\\n",
       "0  2011-10-30T10:15:00+0900  http://news.livedoor.com/article/detail/5978741/   \n",
       "1  2012-02-29T11:45:00+0900  http://news.livedoor.com/article/detail/6322901/   \n",
       "2  2012-01-09T14:00:00+0900  http://news.livedoor.com/article/detail/6176324/   \n",
       "3  2012-05-19T12:00:00+0900  http://news.livedoor.com/article/detail/6573929/   \n",
       "4  2011-10-05T19:11:00+0900  http://news.livedoor.com/article/detail/5914880/   \n",
       "\n",
       "                                 title  \\\n",
       "0  【DVDエンター！】誘拐犯に育てられた女が目にした真実は、孤独か幸福か   \n",
       "1              藤原竜也、中学生とともにロケット打ち上げに成功   \n",
       "2    『戦火の馬』ロイヤル・プレミアにウィリアム王子＆キャサリン妃が出席   \n",
       "3     香里奈、女子高生100人のガチンコ質問に回答「ラーメンも食べる」   \n",
       "4     ユージの前に立ちはだかったJOY「僕はAKBの高橋みなみを守る」   \n",
       "\n",
       "                                                text  label  \n",
       "0   2005年11月から翌2006年7月まで読売新聞にて連載された、直木賞作家・角田光代による...      0  \n",
       "1   「アンテナを張りながら生活をしていけばいい」   2月28日、映画『おかえり、はやぶさ』（...      0  \n",
       "2   3月2日より全国ロードショーとなる、スティーブン・スピルバーグの待望の監督最新作『戦火の馬...      0  \n",
       "3   女優の香里奈が18日、都内で行われた映画『ガール』（5月26日公開）の女子高生限定試写会に...      0  \n",
       "4   5日、東京・千代田区の内幸町ホールにて、映画『キャプテン・アメリカ/ザ・ファースト・アベン...      0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger(\"-Ochasen -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/\")\n",
    "\n",
    "def nn_tokenizer(text):\n",
    "    text = mojimoji.zen_to_han(text.replace(\"\\n\", \"\"), kana=False)\n",
    "    text = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+', \"\", text)\n",
    "    parsed = tagger.parse(text).split(\"\\n\")\n",
    "    parsed = [t.split(\"\\t\") for t in parsed]\n",
    "    parsed = list(filter(lambda x: x[0] != \"\" and x[0] != \"EOS\", parsed))\n",
    "    parsed = [p[2] for p in parsed]\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.tokenize(nn_tokenizer)\n",
    "\n",
    "data = loader.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>concatenated</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-10-30T10:15:00+0900</td>\n",
       "      <td>http://news.livedoor.com/article/detail/5978741/</td>\n",
       "      <td>【DVDエンター！】誘拐犯に育てられた女が目にした真実は、孤独か幸福か</td>\n",
       "      <td>2005年11月から翌2006年7月まで読売新聞にて連載された、直木賞作家・角田光代による...</td>\n",
       "      <td>0</td>\n",
       "      <td>【DVDエンター！】誘拐犯に育てられた女が目にした真実は、孤独か幸福か  2005年11月か...</td>\n",
       "      <td>[【, DVD, エンター, !】, 誘拐犯, に, 育てる, られる, た, 女, が, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-02-29T11:45:00+0900</td>\n",
       "      <td>http://news.livedoor.com/article/detail/6322901/</td>\n",
       "      <td>藤原竜也、中学生とともにロケット打ち上げに成功</td>\n",
       "      <td>「アンテナを張りながら生活をしていけばいい」   2月28日、映画『おかえり、はやぶさ』（...</td>\n",
       "      <td>0</td>\n",
       "      <td>藤原竜也、中学生とともにロケット打ち上げに成功  「アンテナを張りながら生活をしていけばいい...</td>\n",
       "      <td>[藤原竜也, 、, 中学生, とともに, ロケット打ち上げ, に, 成功, 「, アンテナ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-09T14:00:00+0900</td>\n",
       "      <td>http://news.livedoor.com/article/detail/6176324/</td>\n",
       "      <td>『戦火の馬』ロイヤル・プレミアにウィリアム王子＆キャサリン妃が出席</td>\n",
       "      <td>3月2日より全国ロードショーとなる、スティーブン・スピルバーグの待望の監督最新作『戦火の馬...</td>\n",
       "      <td>0</td>\n",
       "      <td>『戦火の馬』ロイヤル・プレミアにウィリアム王子＆キャサリン妃が出席  3月2日より全国ロード...</td>\n",
       "      <td>[『, 戦火の馬, 』, ロイヤル, ・, プレミア, に, ウィリアム王子, &amp;, キャサ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-05-19T12:00:00+0900</td>\n",
       "      <td>http://news.livedoor.com/article/detail/6573929/</td>\n",
       "      <td>香里奈、女子高生100人のガチンコ質問に回答「ラーメンも食べる」</td>\n",
       "      <td>女優の香里奈が18日、都内で行われた映画『ガール』（5月26日公開）の女子高生限定試写会に...</td>\n",
       "      <td>0</td>\n",
       "      <td>香里奈、女子高生100人のガチンコ質問に回答「ラーメンも食べる」  女優の香里奈が18日、都...</td>\n",
       "      <td>[香里奈, 、, 女子高生, 100人, の, ガチンコ!, 質問, に, 回答, 「, ラ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-10-05T19:11:00+0900</td>\n",
       "      <td>http://news.livedoor.com/article/detail/5914880/</td>\n",
       "      <td>ユージの前に立ちはだかったJOY「僕はAKBの高橋みなみを守る」</td>\n",
       "      <td>5日、東京・千代田区の内幸町ホールにて、映画『キャプテン・アメリカ/ザ・ファースト・アベン...</td>\n",
       "      <td>0</td>\n",
       "      <td>ユージの前に立ちはだかったJOY「僕はAKBの高橋みなみを守る」  5日、東京・千代田区の内...</td>\n",
       "      <td>[YU-G, の, 前, に, 立ちはだかる, た, JOY, 「, 僕, は, AKB, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date                                               url  \\\n",
       "0  2011-10-30T10:15:00+0900  http://news.livedoor.com/article/detail/5978741/   \n",
       "1  2012-02-29T11:45:00+0900  http://news.livedoor.com/article/detail/6322901/   \n",
       "2  2012-01-09T14:00:00+0900  http://news.livedoor.com/article/detail/6176324/   \n",
       "3  2012-05-19T12:00:00+0900  http://news.livedoor.com/article/detail/6573929/   \n",
       "4  2011-10-05T19:11:00+0900  http://news.livedoor.com/article/detail/5914880/   \n",
       "\n",
       "                                 title  \\\n",
       "0  【DVDエンター！】誘拐犯に育てられた女が目にした真実は、孤独か幸福か   \n",
       "1              藤原竜也、中学生とともにロケット打ち上げに成功   \n",
       "2    『戦火の馬』ロイヤル・プレミアにウィリアム王子＆キャサリン妃が出席   \n",
       "3     香里奈、女子高生100人のガチンコ質問に回答「ラーメンも食べる」   \n",
       "4     ユージの前に立ちはだかったJOY「僕はAKBの高橋みなみを守る」   \n",
       "\n",
       "                                                text  label  \\\n",
       "0   2005年11月から翌2006年7月まで読売新聞にて連載された、直木賞作家・角田光代による...      0   \n",
       "1   「アンテナを張りながら生活をしていけばいい」   2月28日、映画『おかえり、はやぶさ』（...      0   \n",
       "2   3月2日より全国ロードショーとなる、スティーブン・スピルバーグの待望の監督最新作『戦火の馬...      0   \n",
       "3   女優の香里奈が18日、都内で行われた映画『ガール』（5月26日公開）の女子高生限定試写会に...      0   \n",
       "4   5日、東京・千代田区の内幸町ホールにて、映画『キャプテン・アメリカ/ザ・ファースト・アベン...      0   \n",
       "\n",
       "                                        concatenated  \\\n",
       "0  【DVDエンター！】誘拐犯に育てられた女が目にした真実は、孤独か幸福か  2005年11月か...   \n",
       "1  藤原竜也、中学生とともにロケット打ち上げに成功  「アンテナを張りながら生活をしていけばいい...   \n",
       "2  『戦火の馬』ロイヤル・プレミアにウィリアム王子＆キャサリン妃が出席  3月2日より全国ロード...   \n",
       "3  香里奈、女子高生100人のガチンコ質問に回答「ラーメンも食べる」  女優の香里奈が18日、都...   \n",
       "4  ユージの前に立ちはだかったJOY「僕はAKBの高橋みなみを守る」  5日、東京・千代田区の内...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [【, DVD, エンター, !】, 誘拐犯, に, 育てる, られる, た, 女, が, ...  \n",
       "1  [藤原竜也, 、, 中学生, とともに, ロケット打ち上げ, に, 成功, 「, アンテナ,...  \n",
       "2  [『, 戦火の馬, 』, ロイヤル, ・, プレミア, に, ウィリアム王子, &, キャサ...  \n",
       "3  [香里奈, 、, 女子高生, 100人, の, ガチンコ!, 質問, に, 回答, 「, ラ...  \n",
       "4  [YU-G, の, 前, に, 立ちはだかる, た, JOY, 「, 僕, は, AKB, ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower=True, filters=\"\")\n",
    "tk.fit_on_texts(data.tokenized.map(lambda x: \" \".join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90866"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx = tk.word_index\n",
    "len(word_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(path):\n",
    "    binary = False\n",
    "    if \"bin\" in path:\n",
    "        binary = True\n",
    "    emb = KeyedVectors.load_word2vec_format(path, binary=binary)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanby = load_embedding(\"/Users/arai301070/Data/stanby-jobs-200d-word2vector.bin\")\n",
    "wiki = load_embedding(\"/Users/arai301070/Data/model.vec\")\n",
    "\n",
    "stanby_words = set(stanby.vocab.keys())\n",
    "wiki_words = set(wiki.vocab.keys())\n",
    "livedoor_words = set(word_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48371\n",
      "30477\n"
     ]
    }
   ],
   "source": [
    "oov_stanby = livedoor_words - stanby_words\n",
    "oov_wiki = livedoor_words - wiki_words\n",
    "\n",
    "print(len(oov_stanby))\n",
    "print(len(oov_wiki))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4279\n",
      "22173\n"
     ]
    }
   ],
   "source": [
    "in_stanby = oov_wiki - oov_stanby\n",
    "in_wiki = oov_stanby - oov_wiki\n",
    "\n",
    "print(len(in_stanby))\n",
    "print(len(in_wiki))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorTransformer(nn.Module):\n",
    "    def __init__(self, source_dim, target_dim):\n",
    "        super(VectorTransformer, self).__init__()\n",
    "        self.mat = nn.Linear(source_dim, target_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.mat(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loader(vocab, source_emb, target_emb):\n",
    "    n_vec = len(vocab)\n",
    "    source_dim = source_emb.vector_size\n",
    "    target_dim = target_emb.vector_size\n",
    "\n",
    "    x = np.zeros((n_vec, source_dim))\n",
    "    y = np.zeros((n_vec, target_dim))\n",
    "    for i, key in enumerate(vocab):\n",
    "        source_vec = source_emb.get_vector(key)\n",
    "        target_vec = target_emb.get_vector(key)\n",
    "        x[i, :] = source_vec\n",
    "        y[i, :] = target_vec\n",
    "    x = torch.tensor(x, dtype=torch.float32).to(\"cpu\")\n",
    "    y = torch.tensor(y, dtype=torch.float32).to(\"cpu\")\n",
    "    dataset = data.TensorDataset(x, y)\n",
    "    loader = data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "\n",
    "intersection = stanby_words.intersection(wiki_words)\n",
    "dataloader = create_loader(intersection, stanby, wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VectorTransformer(stanby.vector_size, wiki.vector_size)\n",
    "model.to(\"cpu\")\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for _ in range(3):\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    for (x_batch, y_batch) in dataloader:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item() / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: \n",
      "similar: [('sanjeev', 0.6729212999343872), ('Ekam', 0.6711547374725342), ('kushitani', 0.6677302122116089), ('Yukam', 0.6665024757385254), ('ameblo', 0.6538156270980835), ('belong', 0.653087317943573), ('Maxpoint', 0.6517963409423828), ('Tsuchiya', 0.6465193033218384), ('someone', 0.6425241231918335), ('Com.A', 0.6418524384498596)]\n",
      "word: billings\n",
      "similar: [('typically', 0.8285849690437317), ('exploitation', 0.8231680393218994), ('providing', 0.8215106725692749), ('accordance', 0.8210566639900208), ('mainly', 0.8163630366325378), ('therefore', 0.8141612410545349), ('Ekam', 0.8065015077590942), ('declared', 0.8038038015365601), ('determined', 0.80332350730896), ('suffer', 0.8029685020446777)]\n",
      "word: ユーザエクスペリエンスデザイン\n",
      "similar: [('モデル駆動型アーキテクチャ', 0.7261942625045776), ('xUnit', 0.725232720375061), ('ダイナミックリンクライブラリ', 0.7216801643371582), ('Standardization', 0.7208871841430664), ('ParaView', 0.7192454934120178), ('モデル変換言語', 0.7188775539398193), ('ドメインコントローラ', 0.718255877494812), ('WSDL', 0.7180397510528564), ('リッチインターネットアプリケーション', 0.7162258625030518), ('CardSpace', 0.7149497866630554)]\n",
      "word: ファーマライン\n",
      "similar: [('三菱東京フィナンシャル・グループ', 0.6641286611557007), ('ベインキャピタル', 0.6620704531669617), ('プリンシパル・インベストメンツ', 0.6582026481628418), ('大和証券SMBC', 0.6458470225334167), ('アリコジャパン', 0.6447579860687256), ('セントラルファイナンス', 0.6438466906547546), ('アットローン', 0.6431422233581543), ('サーベラス・キャピタル・マネジメント', 0.6415351033210754), ('リップルウッド・ホールディングス', 0.6388949155807495), ('キヤノンマーケティングジャパン', 0.6377590894699097)]\n",
      "word: 702円\n",
      "similar: [('80665', 0.6546301245689392), ('\\u3000\\u3000\\u3000\\u3000(', 0.6464098691940308), ('0823', 0.6422055959701538), ('03863', 0.6326431632041931), ('0811', 0.6307101249694824), ('0812', 0.6306909322738647), ('0816', 0.6292086839675903), ('0807', 0.6275244951248169), ('0862', 0.6269076466560364), ('landstingsfullmäktige', 0.6257932782173157)]\n",
      "word: 255円\n",
      "similar: [('760円', 0.6501614451408386), ('1100円', 0.6362367272377014), ('0825', 0.6323392391204834), ('216円', 0.631745457649231), ('8500円', 0.6257986426353455), ('985円', 0.6220446825027466), ('2200円', 0.6217662692070007), ('0863', 0.619653582572937), ('2100円', 0.6182587146759033), ('03863', 0.6168469190597534)]\n",
      "word: 名古屋観光専門学校\n",
      "similar: [('サイエンスコミュニケーション', 0.6613082885742188), ('コミュニケーションデザイン', 0.6495892405509949), ('ビジネス・ブレークスルー大学', 0.6466881036758423), ('株式会社ワールド', 0.6458597183227539), ('尚美ミュージックカレッジ専門学校', 0.6455435752868652), ('インフォシス', 0.6448612213134766), ('パブリックリレーションズ', 0.6448111534118652), ('アクレディテーション', 0.6417702436447144), ('コミュニケーションアート', 0.6404885649681091), ('アルバータインターサイエンス', 0.6390531063079834)]\n",
      "word: ＮＥＴ\n",
      "similar: [('リッチインターネットアプリケーション', 0.7733765840530396), ('インターネットセキュリティスイート', 0.7620832920074463), ('アプリケーションストア', 0.7493814826011658), ('ビジネスソフトウェア', 0.7364929914474487), ('ソフトウェア開発キット', 0.7344040870666504), ('ウィキソフトウェア', 0.7280772924423218), ('アプリケーションプログラミングインタフェース', 0.7262305021286011), ('ソフトウェア', 0.7255397439002991), ('ダイナミックリンクライブラリ', 0.7254968285560608), ('PaaS', 0.7209539413452148)]\n",
      "word: deploying\n",
      "similar: [('providing', 0.8259201645851135), ('determined', 0.8201155662536621), ('typically', 0.8151967525482178), ('implementation', 0.8120693564414978), ('exploitation', 0.811350405216217), ('therefore', 0.8097953796386719), ('accordance', 0.8077112436294556), ('implement', 0.8036667108535767), ('consistent', 0.8032763004302979), ('applications', 0.8022534847259521)]\n",
      "word: ①※\n",
      "similar: [('\\u3000\\u3000\\u3000\\u3000(', 0.6353836059570312), ('anthranilate', 0.6146037578582764), ('専修学校高等課程', 0.6055822372436523), ('=−', 0.6049642562866211), ('landstingsfullmäktige', 0.5981869101524353), ('03863', 0.5969629287719727), ('\")-(', 0.5964049100875854), ('Chiropractic', 0.5939642190933228), ('ステート・カレッジ・スパイクス', 0.5935726165771484), ('Nge', 0.5926764011383057)]\n"
     ]
    }
   ],
   "source": [
    "stanby_only_words = stanby_words - intersection\n",
    "for _ in range(10):\n",
    "    word = stanby_only_words.pop()\n",
    "    emb = stanby.get_vector(word)\n",
    "    tensor = torch.tensor(emb, dtype=torch.float32).to(\"cpu\")\n",
    "    pred = model(tensor).detach().numpy()\n",
    "    similar = wiki.similar_by_vector(pred)\n",
    "    print(f\"word: {word}\")\n",
    "    print(f\"similar: {similar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(word):\n",
    "    emb = stanby.get_vector(word)\n",
    "    tensor = torch.tensor(emb, dtype=torch.float32).to(\"cpu\")\n",
    "    pred = model(tensor).detach().numpy()\n",
    "    similar = wiki.similar_by_vector(pred)\n",
    "    pprint.pprint(f\"word: {word}\")\n",
    "    pprint.pprint(f\"similar: {similar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'word: マイクロソフトソリューション'\n",
      "(\"similar: [('しんきん北陸トライアングルネットワークATMサービス', 0.6291667222976685), \"\n",
      " \"('ダイナミックリンクライブラリ', 0.6054505109786987), ('モデル変換言語', 0.6053767800331116), \"\n",
      " \"('xUnit', 0.5980054140090942), ('シングルサインオン', 0.5928266048431396), \"\n",
      " \"('モデル駆動型アーキテクチャ', 0.5893920660018921), ('日開野町', 0.583366870880127), \"\n",
      " \"('ツールチェーン', 0.5811569094657898), ('ビジネスロジック', 0.5804120302200317), \"\n",
      " \"('ポリゴンメッシュ', 0.580378532409668)]\")\n"
     ]
    }
   ],
   "source": [
    "find_similar_words(\"マイクロソフトソリューション\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'word: ユーザエクスペリエンスデザイン'\n",
      "(\"similar: [('モデル駆動型アーキテクチャ', 0.7261942625045776), ('xUnit', \"\n",
      " \"0.725232720375061), ('ダイナミックリンクライブラリ', 0.7216801643371582), \"\n",
      " \"('Standardization', 0.7208871841430664), ('ParaView', 0.7192454934120178), \"\n",
      " \"('モデル変換言語', 0.7188775539398193), ('ドメインコントローラ', 0.718255877494812), ('WSDL', \"\n",
      " \"0.7180397510528564), ('リッチインターネットアプリケーション', 0.7162258625030518), \"\n",
      " \"('CardSpace', 0.7149497866630554)]\")\n"
     ]
    }
   ],
   "source": [
    "find_similar_words(\"ユーザエクスペリエンスデザイン\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'word: スポットクーラー'\n",
      "(\"similar: [('人感センサ', 0.6459254026412964), ('室外機', 0.6403393149375916), \"\n",
      " \"('冷暖房', 0.6274213790893555), ('給湯設備', 0.6240695714950562), ('換気扇', \"\n",
      " \"0.6236535906791687), ('空調', 0.6209656000137329), ('パワーコンディショナー', \"\n",
      " \"0.6202148795127869), ('乗務員宿泊所', 0.6122036576271057), ('ダウンライト', \"\n",
      " \"0.6091240644454956), ('車軸発電機', 0.6078335642814636)]\")\n"
     ]
    }
   ],
   "source": [
    "find_similar_words(\"スポットクーラー\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'word: billings'\n",
      "(\"similar: [('typically', 0.8285849690437317), ('exploitation', \"\n",
      " \"0.8231680393218994), ('providing', 0.8215106725692749), ('accordance', \"\n",
      " \"0.8210566639900208), ('mainly', 0.8163630366325378), ('therefore', \"\n",
      " \"0.8141612410545349), ('Ekam', 0.8065015077590942), ('declared', \"\n",
      " \"0.8038038015365601), ('determined', 0.80332350730896), ('suffer', \"\n",
      " '0.8029685020446777)]')\n"
     ]
    }
   ],
   "source": [
    "find_similar_words(\"billings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'word: ファーマライン'\n",
      "(\"similar: [('三菱東京フィナンシャル・グループ', 0.6641286611557007), ('ベインキャピタル', \"\n",
      " \"0.6620704531669617), ('プリンシパル・インベストメンツ', 0.6582026481628418), ('大和証券SMBC', \"\n",
      " \"0.6458470225334167), ('アリコジャパン', 0.6447579860687256), ('セントラルファイナンス', \"\n",
      " \"0.6438466906547546), ('アットローン', 0.6431422233581543), ('サーベラス・キャピタル・マネジメント', \"\n",
      " \"0.6415351033210754), ('リップルウッド・ホールディングス', 0.6388949155807495), \"\n",
      " \"('キヤノンマーケティングジャパン', 0.6377590894699097)]\")\n"
     ]
    }
   ],
   "source": [
    "find_similar_words(\"ファーマライン\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'word: 選ら'\n",
      "(\"similar: [('ぎふしりつ', 0.645099401473999), ('かみあまくさ', 0.6444453597068787), \"\n",
      " \"('ちばしり', 0.6313422918319702), ('くちよ', 0.6203233003616333), ('あさひまち', \"\n",
      " \"0.6101150512695312), ('むすめどうじょうじ', 0.6086586713790894), ('東京都中央区佃', \"\n",
      " \"0.608351469039917), ('にししん', 0.6080089807510376), ('なごやせん', \"\n",
      " \"0.6051886081695557), ('ろちゅう', 0.6039457321166992)]\")\n"
     ]
    }
   ],
   "source": [
    "find_similar_words(\"選ら\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_loader(vocab, source_emb, target_emb, sample_size=0.6):\n",
    "    n_vec = int(sample_size * len(vocab))\n",
    "    sample_vocab = np.random.choice(list(vocab), \n",
    "                                    size=n_vec, \n",
    "                                    replace=False)\n",
    "    source_dim = source_emb.vector_size\n",
    "    target_dim = target_emb.vector_size\n",
    "\n",
    "    x = np.zeros((n_vec, source_dim))\n",
    "    y = np.zeros((n_vec, target_dim))\n",
    "    for i, key in enumerate(sample_vocab):\n",
    "        source_vec = source_emb.get_vector(key)\n",
    "        target_vec = target_emb.get_vector(key)\n",
    "        x[i, :] = source_vec\n",
    "        y[i, :] = target_vec\n",
    "    x = torch.tensor(x, dtype=torch.float32).to(\"cpu\")\n",
    "    y = torch.tensor(y, dtype=torch.float32).to(\"cpu\")\n",
    "    dataset = data.TensorDataset(x, y)\n",
    "    loader = data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomVectorTransformer:\n",
    "    def __init__(self, output_dim):\n",
    "        self.output_dim = output_dim\n",
    "        self.models = list()\n",
    "        \n",
    "    def fit(self, dataloaders):\n",
    "        for dataloader in dataloaders:\n",
    "            model = VectorTransformer(stanby.vector_size, wiki.vector_size)\n",
    "            model.to(\"cpu\")\n",
    "            optimizer = optim.Adam(model.parameters())\n",
    "            loss_fn = nn.MSELoss()\n",
    "\n",
    "            for _ in range(3):\n",
    "                model.train()\n",
    "                avg_loss = 0.\n",
    "                for (x_batch, y_batch) in dataloader:\n",
    "                    y_pred = model(x_batch)\n",
    "                    loss = loss_fn(y_pred, y_batch)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    avg_loss += loss.item() / len(dataloader)\n",
    "                print(f\"avg_loss: {avg_loss:.3f}\")\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        out = np.zeros(self.output_dim)\n",
    "        n_estimators = len(self.models)\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            out += model(x).detach().numpy() / n_estimators\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = []\n",
    "np.random.seed(10)\n",
    "for _ in range(10):\n",
    "    ld = create_random_loader(intersection, stanby, wiki)\n",
    "    loaders.append(ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvt = RandomVectorTransformer(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.071\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.071\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.071\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.071\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.071\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.071\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.071\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.071\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.071\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.070\n",
      "avg_loss: 0.068\n",
      "avg_loss: 0.068\n"
     ]
    }
   ],
   "source": [
    "rvt.fit(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words_ensemble(word):\n",
    "    emb = stanby.get_vector(word)\n",
    "    tensor = torch.tensor(emb, dtype=torch.float32).to(\"cpu\")\n",
    "    pred = rvt.predict(tensor)\n",
    "    similar = wiki.similar_by_vector(pred)\n",
    "    pprint.pprint(f\"word: {word}\")\n",
    "    pprint.pprint(f\"similar: {similar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'word: ファーマライン'\n",
      "(\"similar: [('プリンシパル・インベストメンツ', 0.7054440379142761), ('ベインキャピタル', \"\n",
      " \"0.7009447813034058), ('日立キャピタル', 0.6900869011878967), ('キヤノンマーケティングジャパン', \"\n",
      " \"0.6856412291526794), ('三菱東京フィナンシャル・グループ', 0.6843117475509644), \"\n",
      " \"('リップルウッド・ホールディングス', 0.6840929985046387), ('アットローン', 0.6791737079620361), \"\n",
      " \"('サーベラス・キャピタル・マネジメント', 0.6786119937896729), ('アリコジャパン', 0.6736608743667603), \"\n",
      " \"('トヨタファイナンス', 0.6711728572845459)]\")\n"
     ]
    }
   ],
   "source": [
    "find_similar_words_ensemble(\"ファーマライン\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'word: スポットクーラー'\n",
      "(\"similar: [('室外機', 0.6489385962486267), ('人感センサ', 0.642612099647522), ('冷暖房', \"\n",
      " \"0.636821448802948), ('パワーコンディショナー', 0.6355054378509521), ('空調', \"\n",
      " \"0.6311885118484497), ('エアコン', 0.6304082870483398), ('車軸発電機', \"\n",
      " \"0.6249876022338867), ('ダウンライト', 0.6181219816207886), ('センタースタンド', \"\n",
      " \"0.6160700917243958), ('受水槽', 0.6158645749092102)]\")\n"
     ]
    }
   ],
   "source": [
    "find_similar_words_ensemble(\"スポットクーラー\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'word: マイクロソフトソリューション'\n",
      "(\"similar: [('しんきん北陸トライアングルネットワークATMサービス', 0.6494717597961426), ('シングルサインオン', \"\n",
      " \"0.6058846116065979), ('xUnit', 0.5999319553375244), ('ダイナミックリンクライブラリ', \"\n",
      " \"0.5992327332496643), ('モデル変換言語', 0.5990589261054993), ('PaaS', \"\n",
      " \"0.5972484350204468), ('ノースゲートビルディング', 0.5944356918334961), ('OMLIS', \"\n",
      " \"0.5944176912307739), ('自動車ターミナル法', 0.594096839427948), \"\n",
      " \"('インターネット・プロトコル・スイート', 0.5927585959434509)]\")\n"
     ]
    }
   ],
   "source": [
    "find_similar_words_ensemble(\"マイクロソフトソリューション\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'word: 選ら'\n",
      "(\"similar: [('ぎふしりつ', 0.6515939831733704), ('かみあまくさ', 0.6432999968528748), \"\n",
      " \"('ちばしり', 0.6325711011886597), ('むすめどうじょうじ', 0.6321752071380615), ('あさひまち', \"\n",
      " \"0.6257833242416382), ('くちよ', 0.6246854066848755), ('ろちゅう', \"\n",
      " \"0.6073383092880249), ('ゅうけいほうそうしょ', 0.6062726974487305), ('ょうえい', \"\n",
      " \"0.6054717302322388), ('みしり', 0.6045638918876648)]\")\n"
     ]
    }
   ],
   "source": [
    "find_similar_words_ensemble(\"選ら\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'word: ユーザエクスペリエンスデザイン'\n",
      "(\"similar: [('ダイナミックリンクライブラリ', 0.7336564064025879), ('xUnit', \"\n",
      " \"0.7298259139060974), ('リッチインターネットアプリケーション', 0.7288536429405212), \"\n",
      " \"('モデル駆動型アーキテクチャ', 0.7276886701583862), ('モデル変換言語', 0.7263566255569458), \"\n",
      " \"('Standardization', 0.7242797613143921), ('ParaView', 0.7225717902183533), \"\n",
      " \"('Liferay', 0.7207211256027222), ('アプリケーションプログラミングインタフェース', \"\n",
      " \"0.7199896574020386), ('DirectWrite', 0.7195067405700684)]\")\n"
     ]
    }
   ],
   "source": [
    "find_similar_words_ensemble(\"ユーザエクスペリエンスデザイン\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
